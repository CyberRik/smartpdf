# ğŸ§  Clinical PDF QA + Summarizer â€” Fullstack RAG App

An end-to-end PDF understanding system built with:

- ğŸ§  **Backend**: RAG (Retrieval-Augmented Generation) w/ LangChain, Transformers, FAISS
- âš›ï¸ **Frontend**: Modern React (TypeScript) + Tailwind + Vite/Next.js (client-side)
- âš™ï¸ **Infrastructure**: Celery for async ingestion, Redis for queueing, Docker-ready

---

## ğŸ“¦ Features

### âœï¸ Upload + Process PDFs
- Upload `.pdf` via drag/drop or button
- Validate + monitor upload progress
- Summary auto-generated after ingestion

### ğŸ’¬ Chat with your PDF
- Ask questions like: *"Whatâ€™s the treatment plan?"*
- Backend uses retrieval from FAISS vectorstore
- Model: `deepset/roberta-base-squad2` via HuggingFace pipeline

### ğŸ§¾ Summarization
- Extracts & chunks text from PDF
- Uses `Falconsai/text_summarization` for chunk-wise summaries

---

## ğŸ§± Project Structure

### ğŸ–¥ Backend (Python)

```
.
â”œâ”€â”€ query.py               # Answer questions from vectorstore
â”œâ”€â”€ ingest.py              # Chunk text + embed into FAISS
â”œâ”€â”€ summarizer.py          # Summarize entire PDF
â”œâ”€â”€ pdf_reader.py          # PDF â†’ text extraction (PyMuPDF)
â”œâ”€â”€ background_tasks.py    # Celery task for ingestion
â”œâ”€â”€ Dockerfile             # Container setup
```

### ğŸ’» Frontend (React/TS)

```
/components
â”œâ”€â”€ UploadArea.tsx         # Drag-and-drop upload UI
â”œâ”€â”€ SummaryBox.tsx         # Beautiful summary display + copy
â”œâ”€â”€ ChatBox.tsx            # Q&A chat interface
â”œâ”€â”€ ChatInput.tsx          # Send questions
â”œâ”€â”€ PDFPreview.tsx         # Show uploaded PDF metadata
â”œâ”€â”€ ProgressBar.tsx        # Upload progress visual
â”œâ”€â”€ StatusIndicator.tsx    # Status badge (Idle, Uploading, etc.)
â”œâ”€â”€ LoadingSkeleton.tsx    # Skeleton UIs
â”œâ”€â”€ TypingIndicator.tsx    # Bot is typing...
```

---

## ğŸš€ Quickstart

### Backend

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Start Redis
redis-server

# 3. Start Celery
celery -A background_tasks worker --loglevel=info

# 4. Start FastAPI or Flask server (not included here)
uvicorn app:app --reload
```

### Frontend

```bash
npm install
npm run dev
```

---

## ğŸŒ API Endpoints

| Endpoint          | Method | Description                          |
|------------------|--------|--------------------------------------|
| `/Uploads`        | POST   | Upload a PDF file                    |
| `/summarize`      | GET    | Get summary of uploaded file         |
| `/ask` (optional) | POST   | Ask a question about uploaded PDF    |

---

## âš™ï¸ Technologies

- ğŸ§  Transformers: `deepset/roberta-base-squad2`, `Falconsai/text_summarization`
- ğŸ—ƒï¸ FAISS vectorstore + LangChain retrievers
- ğŸ‡ Celery + Redis for background ingestion
- ğŸ§© Tailwind CSS + shadcn/ui for frontend polish

---

## ğŸ“Œ Notes

- All summaries and vectorstores are cached locally (`uploads/` + `vectorstores/`)
- PDFs are chunked via `RecursiveCharacterTextSplitter`
- Chat and summarization both rely on existing `.txt` if available for speed

---

## ğŸ™Œ Credits

Built with â¤ï¸ by Ritankar.

